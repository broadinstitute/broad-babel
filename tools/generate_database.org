#+title: Process_data
This notebook shows how to combine all metadata sources into one and generate an sqlite3 database with it. Data should not require manual rearrangements.


* Data acquisition and formatting
First get the metadata sources from JUMP-target-1 and the datasets repo, and remove quotes for InChIKeys
#+begin_src fish
function standard_fields;
    cat - |
    sed "s/\t/,/g" |
    awk '!x{x=sub(/InChIKey/,"standard_key")}1' - |
    sed 's/Metadata_//g' |
    awk '!x{x=sub(/gene/,"standard_key")}1' |
    awk '!x{x=sub(/Symbol/,"standard_key")}1'
end

for perturbation in "compound" "crispr" "orf";
    wget -qO - "https://github.com/jump-cellpainting/datasets/raw/baacb8be98cfa4b5a03b627b8cd005de9f5c2e70/metadata/$perturbation.csv.gz" |
    gzip -d |
    standard_fields > "$perturbation".csv;
    wget -qO - https://github.com/jump-cellpainting/JUMP-Target/raw/bd046851a28fb2257ef4c57c5ea4d496f1a08642/JUMP-Target-1_"$perturbation"_metadata.tsv |
    standard_fields > Target1_$perturbation.csv
end
# And the Target-2 compound metadata
wget -qO - "https://github.com/jump-cellpainting/JUMP-Target/raw/bd046851a28fb2257ef4c57c5ea4d496f1a08642/JUMP-Target-2_compound_metadata.tsv" | standard_fields > Target2_compound.csv;

#+end_src

#+RESULTS:

** Note that some crispr genes occur in other files
And now let us check which entries in crispr.csv exist in other files. It takes a few minutes to run.
#+begin_src bash :noeval
for CRISPR_LINE in $(cat "crispr.csv"); do
    JUMP_ID_CRISPR=$(cut -f1 -d"," <<< $CRISPR_LINE)
    META_SYMBOL=$(cut -f3 -d"," <<< $CRISPR_LINE)
    ORF_LINE=$(grep ",${META_SYMBOL}," orf.csv)
    if [ ! -z "$LINE" ]; then
        JUMP_ID_ORF=$(cut -f1 -d"," <<< $ORF_LINE)
        echo "${JUMP_ID_CRISPR},${JUMP_ID_ORF},${META_SYMBOL}" >> duplicated_crispr_orf.csv
    fi

done
#+end_src

#+RESULTS:

* Combine tables with a similar format/size

** Combine big dataframes (containing JUMP and general ids ).

#+begin_src fish
set fields_csv "JCP2022,standard_key,broad_sample,pert_type,control_type"
set fields (string split , -- $fields_csv)
function clean
   cat - |  tail -n +2  |
   # awk -F"," 'NF < 5{printf $0; for(i = NF; i < 5; ++i)  printf ","; printf "\n" }'
   awk 'BEGIN{FS=OFS=","} {NF=6; print}'
end
echo perturbation,"$fields_csv"  > merged.csv
for i in "compound" "crispr";
csvcut -c (string join ',' $fields[1..2] ) "$i".csv |  sed "s/^/$i,/" | clean >> merged.csv;
end

csvcut -c (string join , $fields[1..4]) orf.csv  | sed 's/^/orf,/' | clean >> merged.csv

head -n 1 merged.csv > merged_small.csv

for i in Target*;
    echo $i;
    set perturbation (basename $i .csv);
    # echo "Analysing $i"
    csvcut -c (string join , $fields[2..5]) $i  | sed "s/^/$perturbation,,/" | clean >> merged_small.csv;
end
#+end_src

#+RESULTS:
| Target1_compound.csv |
| Target1_crispr.csv   |
| Target1_orf.csv      |
| Target2_compound.csv |

** Integrate matches of small datasets with broad_sample to big datasets with JUMP ids
Add the standard_key from the small datasets into the big one
#+BEGIN_SRC jupyter-python :session test
#!/usr/bin/env python3

"""
Combine both data sources, integrating the broad_sample and
additional metadata when available

Assuming
perturbation, jump_id, standard_key, broad_sample for (big) target
perturbation, jump_id, broad_sample, standard_key for source
"""
import csv

target = "merged.csv"
source = "merged_small.csv"

with open(target) as file:
    listed_contents = [x.strip().split(",") for x in file.readlines()]


with open(source) as file:
    source_rows = [x.strip().split(",") for x in file.readlines()]

# perturbation, jump_id, broad_sample,standard_id for target
standard_row = {x[2]: x for x in source_rows[1:]}
for i, target_row in enumerate(listed_contents[1:]):
    new_value_found = standard_row.get(target_row[2])
    if new_value_found:
        listed_contents[i + 1][3:] = new_value_found[3:]
        del standard_row[target_row[2]]

# Add the entries without a jump id
listed_contents += [v for v in standard_row.values()]

# Write into a csv file
with open("entries_synonyms.csv", "w") as f:
    writer = csv.writer(f)
    writer.writerows(listed_contents)
#+END_SRC

#+RESULTS:

** Remove lines with no broad_sample nor standard_id (jump_id by itself is not useful)
#+begin_src bash
# grep -v ",,," "entries_synonyms.csv" > "output.csv"
cp entries_synonyms.csv output.csv
#+end_src

#+RESULTS:

** Save it all as an sql dataset
Use the pandas to save it as a sqlite3 database
#+BEGIN_SRC jupyter-python :session test

import sqlite3
import pandas as pd

csv_file = "output.csv"
conn = sqlite3.connect("./names.db")
c = conn.cursor()
df = pd.read_csv(csv_file)
df.to_sql("names", conn, if_exists="replace", index=False)
#+END_SRC

#+RESULTS:
: 140355



Finally you can upload it to Zenodo and update the pooch retrieval url and hash on [[file:../src/broad_babel/query.py]]
